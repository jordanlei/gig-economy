import numpy as np
import pandas as pd
import math
import io
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import tree
from sklearn.feature_extraction import DictVectorizer


import torch 
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
device =  torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


#%%

boston_rides = pd.read_csv("boston_kaggle/cab_rides.csv")
boston_weather = pd.read_csv("boston_kaggle/weather.csv")

boston_rides = boston_rides[list(set(boston_rides.columns)- set(['product_id','id', 'surge_multiplier']))].dropna()
#%%
def numeric(df, columns):
    for cols in columns:
        df[cols] = pd.Categorical(df[cols])
    return df

def split_data(df, train_split = 0.8):
    length = df.shape[0]
    myrange = np.arange(length)
    np.random.shuffle(myrange)
    train_split = int(train_split * length)
    return df.iloc[myrange[:train_split]].reset_index(drop = True), df.iloc[myrange[train_split:]].reset_index(drop = True)

def labels_data(df, label = "price"):
    x = df[list(set(df.columns) - set([label]))]
    y = df[label]
    return x, y

def accuracy(y, pred):
    return (y == pred)/len(pred)

def rmse(y, pred): 
    return math.sqrt(sum((y-pred)**2))/len(pred)

boston_rides['time_stamp'] = boston_rides['time_stamp']/1e12    
train, test = split_data(boston_rides)
train, val = split_data(train)


class BostonDataset(Dataset):
    """Boston Dataset"""
    def __init__(self, data, transform=None):
        self.data = data
        self.vectorizer = DictVectorizer( sparse = False )
        x, y = labels_data(self.data)
        self.x = self.vectorizer.fit_transform(x.to_dict( orient = 'records' ))
        self.y = y
        
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.y[idx], self.x[idx]

class LinearRegressor(torch.nn.Module):
    def __init__(self, inputSize):
        super(LinearRegressor, self).__init__()
        self.linear = torch.nn.Linear(inputSize, 1)
        torch.nn.init.xavier_uniform_(self.linear.weight)

    def forward(self, x):
        out = self.linear(x)
        return out
    
class Feedforward_1(torch.nn.Module):
    def __init__(self, inputSize):
        super(Feedforward_1, self).__init__()
        self.fc1 = torch.nn.Linear(inputSize, 100)
        self.fc2 = torch.nn.Linear(100, 1)

    def forward(self, x):
        out = F.relu(self.fc1(x))
        out = F.relu(self.fc2(out))
        return out

class Runner():
    def __init__(self, net, criterion, optimizer):
        self.net = net.to(device)
        self.criterion = criterion
        self.optimizer = optimizer

    def train(self, train_load, val_load = None, epochs = 100, log_every = 100):
            self.net = self.net.train()
            step = 0
            
            for epoch in range(epochs):
                for i, (labels, data) in enumerate(train_load, 0):
                    
                    data = Variable(data).to(device).float()
                    labels = Variable(labels).to(device).float().view([-1, 1])
                    
                    #predictions generated by the network
                    y_pred = self.net(data)
                    
                    #criterion evaluated on predictions compared with labels
                    loss = self.criterion(y_pred, labels)
                    
                    #zero gradient the optimizer
                    self.optimizer.zero_grad()
                    
                    #backpropagation
                    loss.backward()
                    
                    #weight update
                    self.optimizer.step()
                                    
                    if (step % log_every==1):
                        print("[Epoch %s] [Step %s] Loss: %.04f"% 
                            (epoch, step, loss.item()))
                    
                    step = step + 1
                    
                if not val_load is None:
                    loss = 0
                    for i, (labels, data) in enumerate(val_load, 0):
                        data = Variable(data).to(device).float()
                        labels = Variable(labels).to(device).float().view([-1, 1])
                                
                        pred = self.net(data)
                        loss += self.criterion(pred, labels)
                        
                    print("Validation MSE: %s"%
                        (loss.item()/i))
                    
    def test(self, test_load):
        self.net = self.net.eval()
        for i, data in enumerate(test_load, 0):
            inputs, labels = data
            inputs, labels = Variable(inputs), Variable(labels)
            
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            inputs = inputs.view(inputs.shape[0], -1)
            
            pred = self.net(inputs)
            _, pred = torch.max(pred, 1)
            
            test_correct = int((pred==labels).sum())
            test_count = labels.size(0)
            print("Test accuracy: %s"%
                (test_correct/test_count))
        return pred, labels, test_correct, test_count

#%%
train_data = BostonDataset(train)
val_data = BostonDataset(val)

trainloader = DataLoader(train_data, batch_size = 64)
valloader = DataLoader(val_data, batch_size = 64)

#%%
model = Feedforward_1(40)
criterion = torch.nn.MSELoss() 
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

runner = Runner(model, criterion, optimizer)
runner.train(trainloader, valloader, epochs= 10)


#%%
for i, (labels, data) in enumerate(trainloader, 0):
    data = Variable(data).to(device).float()
    labels = Variable(labels).to(device).float().view([-1, 1])
    
    #predictions generated by the network
    y_pred = model(data)
    
    #criterion evaluated on predictions compared with labels
    loss = criterion(y_pred, labels)
    
    break